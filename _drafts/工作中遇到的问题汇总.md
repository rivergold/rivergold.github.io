# Python

## numpy数据转化

```python
a = np.zeros((2, 2), np.uint8)
a[0, 0] = 300.11
print(a)
>>> array([[44,  0],
       [ 0,  0]], dtype=uint8)
```

```python
a[0, 0] = 256
print(a)
>>> array([[0, 0],
       [0, 0]], dtype=uint8)
```

**请注意这一点**

如果np.ndarray中的数据大于255，需要转化为整数时，请使用`astype(np.int32)`或者`astype(np.uint32)`。请注意位数，如果使用`astype(np.uint8)`则会出现数据溢出！


## OpenCV build with Cuda

## Numpy `copy` and `view`

***References:***

- [Blog: Numpy Views vs Copies: Avoiding Costly Mistakes](http://www.jessicayung.com/numpy-views-vs-copies-avoiding-costly-mistakes/)

## Path.glob

Path.glob('*') 不会包含`.` `..`

## Python字符串包含双引号

Ref [CSDN: Python中，怎么在字符串里嵌入双引号或者单引号](https://blog.csdn.net/linshenwei1995/article/details/78987444)

## python -m `` 的问题

RuntimeWarning: 'dataset.reg_dataset' found in sys.modules after import of package 'dataset', but prior to execution of 'dataset.reg_dataset'; this may result in unpredictable behaviour
  warn(RuntimeWarning(msg))

自己总结的经验：
- 如果想要执行package中的module，使用`python -m <pakcage name>.<module_name>`：
   - 请不要在package中的__init__.py中import这个module，否则该module会被执行两遍：第一次实在import这个package时，第二次是你的执行package.module时，这是还会出现`RuntimeWarning: 'dataset.reg_dataset' found in sys.modules after import of package 'dataset', but prior to execution of 'dataset.reg_dataset'; this may result in unpredictable behaviour
  warn(RuntimeWarning(msg))`的warning

Ref [Medium: Python 的 Import 陷阱](https://medium.com/pyladies-taiwan/python-%E7%9A%84-import-%E9%99%B7%E9%98%B1-3538e74f57e3)


### Script 与 module的区别

本质上没有区别，只是script会有`__name__ == '__main__'`，而module一般是被别的文件import的

Ref [stackoverflow: What is the difference between a module and a script in Python?](https://stackoverflow.com/questions/2996110/what-is-the-difference-between-a-module-and-a-script-in-python)

# shell

## bash与sh区别

`test.sh`:

```shell
#!/bin/sh

a=10
b=20

if [ $a == $b ]
then
   echo "a is equal to b"
else
   echo "a is not equal to b"
fi
```

When run `sh test.sh`, occur `run.sh: 6: [: 10: unexpected operator`

`sh` not support `==`

Ref [stackoverflow: [:Unexpected operator in shell programming [duplicate]](https://stackoverflow.com/a/3411105/4636081)

`#!/bin/bash` will work when run `./test.sh`

<!--  -->
<br>

***

<br>
<!--  -->

# Android开发

## `Build APKS`操作可以实现对动态库进行“抽取”

原本编译出来的libtensorflowlite有50多MB，进行build apks生成的apk中包含的动态库只有几百k。

单独使用CMake，脱离android studio编译出来的库也可以通过build apks实现动态库的抽取

<!--  -->
<br>

***

<br>
<!--  -->

# 变量命名
- 对于带修饰词的变量，i.e `img_scaled`， 将修饰词放置后面
- 一般变量，使用组合名词表示

# Other

编写visualier时，需要将tensor转化为numpy，出现error
```
Traceback (most recent call last):
  File "train.py", line 121, in <module>
    train(opt)
  File "train.py", line 107, in train
    visualizer.plot_images(x, y_hat)
  File "src/utils/visualizer.py", line 105, in plot_images
    kps = tensor2kps(kps_tensor[s_id])
  File "src/utils/utils.py", line 67, in tensor2kps
    kps = kps.numpy().reshape(-1, 2)
RuntimeError: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.
```

`detach` do what:

Ref [CSDN: pytorch: Variable detach 与 detach_](https://blog.csdn.net/u012436149/article/details/76714349): 该链接所写的，值得研究！

- [PyTorch doc: `torch.Tensor`](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach)

<br>

***

<br>

## git ignore file by path

我的repo里面有两个名叫dataset的文件夹，我想忽略根目录下的。如果在`.gitignore`写`dataset/`会将两个都忽略，需要改写为`/dataset/`来根据路径进行忽略

ref [segmentfault: .gitignore 如何仅忽略根目录下的index.html](https://segmentfault.com/q/1010000004064190)

## 3D模型基本概念

- [Unity doc: Materials, Shaders & Textures](https://docs.unity3d.com/Manual/Shaders.html)

- [AUTODESK MAYA 2017 帮助](http://help.autodesk.com/view/MAYAUL/2017/CHS/?guid=GUID-FDCD0C68-2496-4405-A785-3AA93E9A3B25)

<br>

***

<br>

# Python提升GPU利用率

## 减少花费在CPU上的数据预处理时间
  
### 使用多核CPU处理

Python get cpu count:

- `os.cpu_count()`
- `multiprocessing.cpu_count()`

Ref [stackoverflow: How to find out the number of CPUs using python](https://stackoverflow.com/a/25636145/4636081)

给PyTorch中的Dataloader设置`num_workers`

Ref [PyTorch Forum: How to speed up the data loader](https://discuss.pytorch.org/t/how-to-speed-up-the-data-loader/13740/6?u=rivergold)

- [ ] `pin_memory=True` 细节；测试时，1w的数据集没有明显提升GPU使用率

## 使用HDF5存储训练数据

Ref [PyTorch Forum: How to speed up the data loader](https://discuss.pytorch.org/t/how-to-speed-up-the-data-loader/13740/3?u=rivergold)

- [ ] 有待调研

## 有用的资料

- [Medium: Speed Up your Algorithms Part 1 — PyTorch](https://towardsdatascience.com/speed-up-your-algorithms-part-1-pytorch-56d8a4ae7051)
- [SAGIVTECH: Optimizing PyTorch training code](https://www.sagivtech.com/2017/09/19/optimizing-pytorch-training-code/)

<br>

***

<br>

# Visdom

## Visdom save and reshow

### Visdom save `env` as `.json`

<p align="center">
  <img
  src="https://upload-images.jianshu.io/upload_images/9890707-6a8de499029d7001.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="100%">
</p>

### Visdom reshow from `.json`

```bash
$ visdom -env_path <folder contain .json>
```

Ref [Github facebookresearch: visdom](https://github.com/facebookresearch/visdom#usage)

<br>

***

<br>

# ssh

## [建议]使用`alias`建立登陆远程虚拟机的的快捷键
```bash
alias="ssh root@<ip>"
```

## Use zssh

### Install

- Ubuntu: `sudo apt install zssh`

Ref [博客园: linux 机器之间 zssh, rz, sz互相传输](https://www.cnblogs.com/strikebone/p/3454679.html)

### Use

#### Upload

1. `ctr + @`: 切换到local
2. `sz <file>`: 从local上传文件到remote

#### Download

1. `sz <file>` 在remote启动sz，准备发送文件
2. `ctr + @`: 切换到local
3. `cd <path>`: 进入到需要存储的路径
4. `rz`: 接受文件

***References:***

- [Blog: zssh使用](https://phenix3443.github.io/notebook/ubuntu/zssh.html)

<!--  -->
<br>

***

<br>
<!--  -->

# `rm`与`trash`

`rm`操作存在风险，建议使用`trash`替代`rm`

最好在`~/.zshrc`中配置：

```bash
alias rm='echo "rm is disabled, use trash or /bin/rm instead."
```

Ref [CommandLinefu.com: Disable rm, use trash instead](https://www.commandlinefu.com/commands/view/10118/disable-rm-use-trash-instead)

之后安装`trash-cli`

```bash
$ sudo apt install trash-cli
```

Ref [Linux公社：Ubuntu命令行的垃圾箱Trash CLI，远离 rm 命令误删除重要文件的阴影](https://www.linuxidc.com/Linux/2018-11/155406.htm)

## Where is `Trash` location

- User trash directory is `~/.local/share/Trash`

- If you deleted something as root: `/root/.local/share/Trash`

Ref [StackExchange: Where is the .Trash folder?](https://askubuntu.com/questions/102099/where-is-the-trash-folder)

## trash-cli基本使用

- `trash <file or folder>`: trash file or folder into trash

- `trash-restore`: resotre file deleted from current folder

# `zip` and `7z`

## Zip many files into several archives

***References:***

- [StackExchange: Zip many files into several archives](https://superuser.com/a/602711)
- [StackExchange: How could I portably split large backup files over multiple discs?](https://superuser.com/questions/602735/how-could-i-portably-split-large-backup-files-over-multiple-discs/602736#602736)

# `fd`

Ref [简书: fd - 更好的 find 命令](https://www.jianshu.com/p/cfe79da128bc)

# 动态库信息查看

```bash
$ readelf -d <.so> | grep 'NEEDED'
```

Ref [StackExchange: How to find out the dynamic libraries executables loads when run?](https://unix.stackexchange.com/a/220110)

# `pip`

`pip install`: 没有`-y`的参数
`pip uninstall`: 有`-y`的参数

# git

## 合并单个commit

use `cherry-pick`

## 合并多个commit

- [ ] 有待研究

Gitkraken中使用`Squash`实现。
Ref [GitKraken doc: Squash](https://support.gitkraken.com/working-with-commits/squash/).

**注意:** 如果remote如果已经有了你需要合并的commit记录，那只能只用`force push`来达到remote修改历史commit的目的。请谨慎操作

Ref [CSDN: Git合并特定commits 到另一个分支](https://blog.csdn.net/ybdesire/article/details/42145597)

# 评价标注

## Accuracy, Precision and Recall

Ref [简书: 准确率(Accuracy) | 查准率(Precision) | 查全率(Recall)](https://www.jianshu.com/p/8b7324b0f307)

## map

Ref [知乎: 目标检测中的mAP是什么含义？](https://www.zhihu.com/question/53405779/answer/419532990)

## P-R曲线

Ref [简书: 二战周志华《机器学习》-PR曲线和ROC曲线](https://www.jianshu.com/p/75a163a17fb5)

## TP, TN, FP, FN

Ref [CSDN: TP FN FP TN](https://blog.csdn.net/power0405hf/article/details/50421088)

<!--  -->
<br>

***

<br>
<!--  -->

# Python `argparse`

[Here is an example.](https://gist.github.com/rivergold/381fda065a4d36d17645e96427ddf878)

## Can not use `sys.argv`

使用`parser.parse_args(args)`可以从`['value', 'key1', 'value1', ...]`中进行解析

Ref [stackoverflow: Python error: the following arguments are required](https://stackoverflow.com/questions/51172076/python-error-the-following-arguments-are-required)

## `add_subparsers`

- [ ] 有待深入了解；需求：一个参数根据另一个参数出现会作为必要选项

***References:***

- [kite: add_subparsers](https://kite.com/python/docs/argparse.ArgumentParser.add_subparsers)
- [stackoverflow: Python Argparse - conditionally required arguments based on the value of another argument](https://stackoverflow.com/questions/52086550/python-argparse-conditionally-required-arguments-based-on-the-value-of-another)

# Warm start

> 值得一提的是模型的热启动（warm start）问题。热启动解决的是模型需要长时间训练才能收敛的问题，这个时候可以用一个已经训练好的模型，从中选取一些共同的特征权重，作为模型的初始状态，这样可以使得模型更快收敛，从而加快训练速度。

Ref [知乎: 一分钟整明白Tensorflow Extended](https://zhuanlan.zhihu.com/p/31041536)

- [ ] warm up的含义

<!--  -->
<br>

***

<br>
<!--  -->

# Pytorch

## `optimizer`

- `param_groups`: `list`
- `param_groups[0]`: `dict`

```python
for param_group in param_groups:
    print(type(param_group))
>>> <class 'dict'>
```

- [ ] Adam optimizer的参数: `dict_keys(['lr', 'betas', 'eps', 'weight_decay', 'amsgrad', 'params'])`

## `Conv`

```python
conv = torch.nn.Conv2d(3, 24, 3)
conv.weight.size()
>>> torch.Size([24, 3, 3, 3])
#

conv = torch.nn.Conv2d(3, 24, (4, 5))
conv.weight.size()
>>> torch.Size([24, 3, 4, 5])
```

conv2d weight tensor size: `[C_out, C_in, K_h, K_w]`

# 模型重新训练

训练是需要存储的有：

- 网络的参数
- 优化器的参数
- lr-scheduler的参数
- 当前的步数（主要用于显示、lr-decay的计算；但一般lr-sheduler中会自己计数）

使用以上的参数基本可以恢复当时的训练场景。

如果想要warm-start（在之前训练的checkpoint的基础之上，调整优化策略、lr decay的方式）的话：restore以下参数

- 模型参数
- 优化器参数（如果没有更换优化器）

之后重新配置lr-scheduler

即可