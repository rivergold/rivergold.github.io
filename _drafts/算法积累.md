# Base

## P, NP, NP-Complelte and NP-Hard

***Referecne:*** [stackoverflow: What are the differences between NP, NP-Complete and NP-Hard?](https://stackoverflow.com/questions/1857244/what-are-the-differences-between-np-np-complete-and-np-hard)

<!--  -->
<br>

***
<!--  -->

## 堆排序

***References:***

- [CSDN: 堆树（最大堆、最小堆）详解](https://blog.csdn.net/guoweimelon/article/details/50904346)

- [Blog: 堆排序的时间复杂度](https://chihminh.github.io/2016/08/08/heap-sort/)

<!--  -->
<br>

***
<!--  -->

## 多叉树遍历

- 广度优先遍历: 使用队列
- 深度优先遍历: 使用栈

<!--  -->
<br>

***
<!--  -->

## 凸包

<!--  -->
<br>

***
<!--  -->

## Automatic Differentiation

计算机实现求导目前主要有两种方法：
- 基于符号的算术求导: 构建计算图
- 直接用数值进行求导: 使用求导公式，一般只用来检验求导结果

***Ref*** [Code码农网: CSE 599W： Systems for ML](https://www.codercto.com/a/29673.html) and [CSE599W: Lecture 4: Backpropagation and
Automatic Differentiation](http://dlsys.cs.washington.edu/pdf/lecture4.pdf)

***Referneces:***

- [知乎: tensorflow的函数自动求导是如何实现的？](https://www.zhihu.com/question/54554389/answer/164942272)

- [Github: dlsys-course/assignment1](https://github.com/dlsys-course/assignment1)

- [CSE 599W](http://dlsys.cs.washington.edu/pdf/lecture4.pdf)

### Tools

- [autograd](https://github.com/HIPS/autograd)

***References:***

- [ResearchGate: Is there an efficient automatic differentiation package in Python?](https://www.researchgate.net/post/Is_there_an_efficient_automatic_differentiation_package_in_Python)
- [autodiff.org](http://www.autodiff.org/?module=Tools&language=python)

<!--  -->
<br>

***
<!--  -->

## NMS

***Ref:*** [博客园: 非极大值抑制（NMS）的几种实现](https://www.cnblogs.com/king-lps/p/9031568.html)

<!--  -->
<br>

***
<!--  -->

## Linear Computation

### Tools

#### C++

- Better choose `Eigen` first

***References:***

- [知乎: 矩阵运算库blas, cblas, openblas, atlas, lapack, mkl之间有什么关系，在性能上区别大吗？](https://www.zhihu.com/question/27872849)
- [Blog: C++线性运算库梳理](https://milkpku.github.io/blog/2017/12/15/C++%E7%BA%BF%E6%80%A7%E8%BF%90%E7%AE%97%E5%BA%93%E6%A2%B3%E7%90%86/)


<!--  -->
<br>

***

<br>
<!--  -->

# Optimization

## Tools

- [Google OR-Tools](https://github.com/google/or-tools)

***Ref:*** [Github Awesome Optimization](https://github.com/jkerfs/awesome-optimization)

## Books

- [Convex Optimization](http://stanford.edu/~boyd/cvxbook/)

## Heuristic

- [Wiki: Heuristic (computer science)](https://en.wikipedia.org/wiki/Heuristic_(computer_science))

### Evolutionary Optimization

- [Pyswarm](https://pypi.org/project/pyswarm/): a gradient-free, evolutionary optimization package for python that supports constraints.

- [PySwarms](https://pyswarms.readthedocs.io/en/latest/): an extensible research toolkit for particle swarm optimization (PSO) in Python.

<!--  -->
<br>

***

<br>
<!--  -->

# Machine Learning

## Stacking

***References:*** [简书: Kaggle提升模型性能的超强杀招Stacking——机器学习模型融合](https://www.jianshu.com/p/719fc024c0ec)

<!--  -->
<br>

***

<br>
<!--  -->

# Deep Learning

## Basics

### How to calculate gradient in deep learning?

Key: use computation graphs.

使用计算图表示计算过程，在计算梯度时，基于链式求导法则，构建梯度的计算图。

**Computation Graphs:**

- [PyTorch Computation Graph](https://jdhao.github.io/2017/11/12/pytorch-computation-graph/)

<!--  -->
<br>

***
<!--  -->

### Image Dataset RGB Mean

Calculate all training set to get R, G, B means.

Ref [Github DrSleep/tensorflow-deeplab-resnet: How can I get IMG_MEAN of a custom dataset? #146](https://github.com/DrSleep/tensorflow-deeplab-resnet/issues/146)

<!--  -->
<br>

***

<br>
<!--  -->

# Object Detection

## Survey

- [TPAMI: Object Detection in 20 Years: A Survey](https://arxiv.org/pdf/1905.05055.pdf)

***References:***

- [专知: 密歇根大学40页《20年目标检测综述》最新论文，带你全面了解目标检测方法](https://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247511243&idx=1&sn=f2f7d013f4d7704eee3f0f94ffc1fbd1&chksm=fc864dd8cbf1c4ce4ef226d5f7c5a58b8ee9ccadfbd584e548154266f947d6e5dd7bfb6a25ed&mpshare=1&scene=1&srcid=#rd)

<!--  -->
<br>

***
<!--  -->

## SSD

Code:

- [Github qfgaohao/pytorch-ssd](https://github.com/qfgaohao/pytorch-ssd)
- [Github amdegroot/ssd.pytorch](https://github.com/amdegroot/ssd.pytorch)

<!--  -->
<br>

***

<br>
<!--  -->

# AutoML

## Survey

- [JMLR: Survey on Automated Machine Learning](https://arxiv.org/abs/1904.12054)

***References:***

- [机器之心: AutoML研究综述：让AI学习设计AI](https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&mid=2247489146&idx=4&sn=e3a45136cd9149ce08d81ece80dcddb8&chksm=f9a264f5ced5ede3e65cca95144cfa6fc789b3c605257c5ed4b9d1fa9efbd4d24868fefba141&mpshare=1&scene=1&srcid=#rd)
- [机器之心: 业界 | 进化算法 + AutoML，谷歌提出新型神经网络架构搜索方法](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739385&idx=2&sn=592ef223e200fc91ecdc787b6f4bc0b8&scene=21#wechat_redirect)

<!--  -->
<br>

***

<br>
<!--  -->

# 思想总结

- 拉格朗日乘数法：将约束的优化问题，通过一系列的变换，转化为无约束的问题，从而便于求解。
