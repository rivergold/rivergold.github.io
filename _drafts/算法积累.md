# Base

## P, NP, NP-Complelte and NP-Hard

**_Referecne:_** [stackoverflow: What are the differences between NP, NP-Complete and NP-Hard?](https://stackoverflow.com/questions/1857244/what-are-the-differences-between-np-np-complete-and-np-hard)

<!--  -->
<br>

---

<!--  -->

## 堆排序

**_References:_**

- [CSDN: 堆树（最大堆、最小堆）详解](https://blog.csdn.net/guoweimelon/article/details/50904346)

- [Blog: 堆排序的时间复杂度](https://chihminh.github.io/2016/08/08/heap-sort/)

<!--  -->
<br>

---

<!--  -->

## 树

### 多叉树遍历

- 广度优先遍历: 使用队列
- 深度优先遍历: 使用栈

<!--  -->
<br>

---

<!--  -->

## 图

### 最短路径

#### Bellman-Ford

**_References:_**

- [掘金: 算法(五):图解贝尔曼-福特算法](https://juejin.im/post/5b77fec1e51d4538cf53be68)
- [博客园: Bellman-Ford 单源最短路径算法](https://www.cnblogs.com/gaochundong/p/bellman_ford_algorithm.html)

<!--  -->
<br>

---

<!--  -->

## 凸包

<!--  -->
<br>

---

<!--  -->

## Geodesic Distance

**_References:_**

- [Github: taigw/geodesic_distance](https://github.com/taigw/geodesic_distance)

<!--  -->
<br>

---

<!--  -->

## Automatic Differentiation

计算机实现求导目前主要有两种方法：

- 基于符号的算术求导: 构建计算图
- 直接用数值进行求导: 使用求导公式，一般只用来检验求导结果

**_Ref_** [Code 码农网: CSE 599W： Systems for ML](https://www.codercto.com/a/29673.html) and [CSE599W: Lecture 4: Backpropagation and
Automatic Differentiation](http://dlsys.cs.washington.edu/pdf/lecture4.pdf)

**_Referneces:_**

- [知乎: tensorflow 的函数自动求导是如何实现的？](https://www.zhihu.com/question/54554389/answer/164942272)

- [Github: dlsys-course/assignment1](https://github.com/dlsys-course/assignment1)

- [CSE 599W](http://dlsys.cs.washington.edu/pdf/lecture4.pdf)

### Tools

- [autograd](https://github.com/HIPS/autograd)

**_References:_**

- [ResearchGate: Is there an efficient automatic differentiation package in Python?](https://www.researchgate.net/post/Is_there_an_efficient_automatic_differentiation_package_in_Python)
- [autodiff.org](http://www.autodiff.org/?module=Tools&language=python)

<!--  -->
<br>

---

<!--  -->

## Bias and Variance

```python
from numpy.linalg import norm
n_samples = 20
f_x, f_y = f(n_samples)
n_models = 100
max_degree = 15
var_vals =[]
bias_vals = []
error_vals = []
for degree in xrange(1, max_degree):
    avg_y = np.zeros(n_samples)
    models = []
    for i in xrange(n_models):
        (x,y) = sample(n_samples)
        model = fit_polynomial(x, y, degree)
        p_y = apply_polynomial(model, x)
        avg_y = avg_y + p_y
        models.append(p_y)
    avg_y = avg_y / n_models
    bias_2 = norm(avg_y - f_y)/f_y.size
    bias_vals.append(bias_2)
    variance = 0
    for p_y in models:
        variance += norm(avg_y - p_y)
    variance /= f_y.size * n_models
    var_vals.append(variance)
    error_vals.append(variance + bias_2)
pl.plot(range(1, max_degree), bias_vals, label='bias')
pl.plot(range(1, max_degree), var_vals, label='variance')
pl.plot(range(1, max_degree), error_vals, label='error')
pl.legend()
```

**_Ref:_** [博客园: 非极大值抑制（NMS）的几种实现](https://www.cnblogs.com/king-lps/p/9031568.html)

<!--  -->
<br>

---

<!--  -->

## Linear Computation

### Tools

#### C++

- Better choose `Eigen` first

**_References:_**

- [知乎: 矩阵运算库 blas, cblas, openblas, atlas, lapack, mkl 之间有什么关系，在性能上区别大吗？](https://www.zhihu.com/question/27872849)
- [Blog: C++线性运算库梳理](https://milkpku.github.io/blog/2017/12/15/C++%E7%BA%BF%E6%80%A7%E8%BF%90%E7%AE%97%E5%BA%93%E6%A2%B3%E7%90%86/)

<!--  -->
<br>

---

<br>
<!--  -->

# Optimization

## Tools

- [Google OR-Tools](https://github.com/google/or-tools)

**_Ref:_** [Github Awesome Optimization](https://github.com/jkerfs/awesome-optimization)

## Books

- [Convex Optimization](http://stanford.edu/~boyd/cvxbook/)

## Heuristic

- [Wiki: Heuristic (computer science)](<https://en.wikipedia.org/wiki/Heuristic_(computer_science)>)

### Evolutionary Optimization

- [Pyswarm](https://pypi.org/project/pyswarm/): a gradient-free, evolutionary optimization package for python that supports constraints.

- [PySwarms](https://pyswarms.readthedocs.io/en/latest/): an extensible research toolkit for particle swarm optimization (PSO) in Python.

<!--  -->
<br>

---

<br>
<!--  -->

# Similarity

## 分布的相似度

- [知乎: 分布的相似度（距离）用什么模型比较好？](https://www.zhihu.com/question/39872326)

### Wasserstein distance

```python
from scipy.stats import wasserstein_distance
similarity = wasserstein_distance(u_values, v_values)
```

**_References:_**

- [CSDN: 关于使用 OpenCV 求解 wasserstein distance 的一些问题及解决方法](https://blog.csdn.net/weixin_43439979/article/details/88870795)

<!--  -->
<br>

---

<br>
<!--  -->

# Image Processing

## Resize Image

Here are 5 methods in OpenCV to resize image:

- `INTER_NEAREST`
- `INTER_LINEAR`
- `INTER_AREA`
- `INTER_CUBIC`
- `INTER_LANCZOS4`

When to use:

- Shrink image: `INTER_AREA` looks best
- Zoom image: `CV_INTER_CUBIC` looks best but slow, `CV_INTER_LINEAR` faster but still looks ok.

**_Ref:_** [OpenCV Doc: resize](https://docs.opencv.org/3.4.6/da/d54/group__imgproc__transform.html#ga47a974309e9102f5f08231edc7e7529d)

**_References:_**

- [stackoverflow: How to Resize image without loosing image quality in c++ or opencv](https://stackoverflow.com/questions/33183272/how-to-resize-image-without-loosing-image-quality-in-c-or-opencv)

<!--  -->
<br>

---

<!--  -->

## Integral image

**理解:** 主对角线的和 减去 次对角线的和

**_Ref:_** [Wiki: 积分图](https://zh.wikipedia.org/wiki/%E7%A7%AF%E5%88%86%E5%9B%BE)

<!--  -->
<br>

---

<br>
<!--  -->

# Machine Learning

## Awesome code

### numpy-ml

- [Github](https://github.com/ddbourgin/numpy-ml)

**_Ref:_** [机器之心: 惊为天人，NumPy 手写全部主流机器学习模型，代码超 3 万行](https://zhuanlan.zhihu.com/p/72331050?utm_source=ZHShareTargetIDMore&utm_medium=social&utm_oi=37839882420224)

<!--  -->
<br>

---

<!--  -->

## Bias and Variance

```python
from numpy.linalg import norm
n_samples = 20
f_x, f_y = f(n_samples)
n_models = 100
max_degree = 15
var_vals =[]
bias_vals = []
error_vals = []
for degree in xrange(1, max_degree):
    avg_y = np.zeros(n_samples)
    models = []
    for i in xrange(n_models):
        (x,y) = sample(n_samples)
        model = fit_polynomial(x, y, degree)
        p_y = apply_polynomial(model, x)
        avg_y = avg_y + p_y
        models.append(p_y)
    avg_y = avg_y / n_models
    bias_2 = norm(avg_y - f_y)/f_y.size
    bias_vals.append(bias_2)
    variance = 0
    for p_y in models:
        variance += norm(avg_y - p_y)
    variance /= f_y.size * n_models
    var_vals.append(variance)
    error_vals.append(variance + bias_2)
pl.plot(range(1, max_degree), bias_vals, label='bias')
pl.plot(range(1, max_degree), var_vals, label='variance')
pl.plot(range(1, max_degree), error_vals, label='error')
pl.legend()
```

Ref [Github Gist: fabgoos/Bias and Variance.ipynb](https://gist.github.com/fabgoos/6788818)

**_References:_**

- [Liuchengxu Blog: 偏差与方差](http://liuchengxu.org/blog-cn/posts/bias-variance/)
- [Blog: Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)
- [知乎: 机器学习中的 Bias 和 Variance](https://zhuanlan.zhihu.com/p/45213397)
- [Blog: 谈谈 Bias-Variance Tradeoff](https://liam.page/2017/03/25/bias-variance-tradeoff/)

<!--  -->
<br>

---

<!--  -->

## Ensemble learning

**_References:_**

- [知乎: 集成学习三大法宝-bagging、boosting、stacking](https://zhuanlan.zhihu.com/p/36161812)
- [Medium: Boosting, Bagging, and Stacking — Ensemble Methods with sklearn and mlens](https://medium.com/@rrfd/boosting-bagging-and-stacking-ensemble-methods-with-sklearn-and-mlens-a455c0c982de)

## Stacking

**_References:_** [简书: Kaggle 提升模型性能的超强杀招 Stacking——机器学习模型融合](https://www.jianshu.com/p/719fc024c0ec)

<!--  -->
<br>

---

<!--  -->

## HMM, CRF

**_Ref:_** [知乎: 概率图模型体系：HMM、MEMM、CRF](https://zhuanlan.zhihu.com/p/33397147)

**_References:_**

- [知乎: HMM 和 CRF 区别](https://zhuanlan.zhihu.com/p/31187060)
- [知乎: 如何用简单易懂的例子解释条件随机场（CRF）模型？它和 HMM 有什么区别？](https://www.zhihu.com/question/35866596)

<!--  -->
<br>

---

<br>
<!--  -->

# Deep Learning

## Survey

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)

## Basics

### How to calculate gradient in deep learning?

Key: use computation graphs.

使用计算图表示计算过程，在计算梯度时，基于链式求导法则，构建梯度的计算图。

**Computation Graphs:**

- [PyTorch Computation Graph](https://jdhao.github.io/2017/11/12/pytorch-computation-graph/)

### Convolution Implemention

- [知乎: 在 Caffe 中如何计算卷积？](https://www.zhihu.com/question/28385679)

Input: $(N, C_{in}, H_{in}, W_{in})$
Output: $(N, C_{out}, H_{out}, W_{out})$
Kernel size: $(k, k)$

$$
H_{out} = \frac{H_{in} - 2p - k}{s} + 1
$$

So, if when $s = 1$, and want $H_{out} = H_{in}$,

$$
2p = k - 1
$$

$$
p = \frac{k - 1}{2}
$$

**_References:_**

- [Ldy's Blog: Transposed Convolution, Fractionally Strided Convolution or Deconvolution](https://buptldy.github.io/2016/10/29/2016-10-29-deconv/)
- [AI 研习社: 使用转置卷积进行上采样](https://ai.yanxishe.com/page/TextTranslation/856)
- [PyTorch Forum: Padding setting of torch.nn.ConvTranspose2d](https://discuss.pytorch.org/t/padding-setting-of-torch-nn-convtranspose2d/21066)

### Image Dataset RGB Mean

Calculate all training set to get R, G, B means.

Ref [Github DrSleep/tensorflow-deeplab-resnet: How can I get IMG_MEAN of a custom dataset? #146](https://github.com/DrSleep/tensorflow-deeplab-resnet/issues/146)

## Loss

### Cross-Entropy

**_References:_**

- [CSDN: 理解交叉熵作为损失函数在神经网络中的作用](https://blog.csdn.net/chaipp0607/article/details/73392175)

- [CSDN: 一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉](https://blog.csdn.net/tsyccnh/article/details/79163834)

- [Wiki: 交叉熵](https://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E7%86%B5)

**Code:**

```python
import torch
import torch.nn.functional as F
import numpy as np
y = torch.from_numpy(np.array([0]))
y_hat = torch.from_numpy(np.array([[0.9, 0.1, 0]]))
F.cross_entropy(y_hat, y)
>>> tensor(0.6184, dtype=torch.float64)
```

**_Ref:_** [PyTorch Doc torch.nn: cross_entropy](https://pytorch.org/docs/stable/nn.html#cross-entropy)

<!--  -->
<br>

---

<!--  -->

## Awesome Tools

- [PyTorch Hub](https://github.com/pytorch/hub)

<!--  -->
<br>

---

<br>
<!--  -->

# Object Detection

## Survey

- [TPAMI: Object Detection in 20 Years: A Survey](https://arxiv.org/pdf/1905.05055.pdf)

**_References:_**

- [专知: 密歇根大学 40 页《20 年目标检测综述》最新论文，带你全面了解目标检测方法](https://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247511243&idx=1&sn=f2f7d013f4d7704eee3f0f94ffc1fbd1&chksm=fc864dd8cbf1c4ce4ef226d5f7c5a58b8ee9ccadfbd584e548154266f947d6e5dd7bfb6a25ed&mpshare=1&scene=1&srcid=#rd)

## Dataset

- [COCO](http://cocodataset.org/#download)

## Tool

### MMDetection

- [Github](https://github.com/open-mmlab/mmdetection)

## Blog

- [知乎-Wang Naiyan 聊聊 Anchor 的"前世今生"（上）](https://zhuanlan.zhihu.com/p/63273342)
- [知乎-Wang Naiyan 聊聊 Anchor 的"前世今生"（下）](https://zhuanlan.zhihu.com/p/68291859)
- [知乎: Focal loss 论文详解](https://zhuanlan.zhihu.com/p/49981234)

<!--  -->
<br>

---

<!--  -->

## Basic Concept

**Accuracy**

$$
Accuracy = \frac{TruePositive + TrueNegative}{AllSamples}
$$

**Precision**

For a specific class,

$$
Precision = \frac{TruePositives}{AllDetections} = \frac{TP}{TP + FP}
$$

**Recall**

$$
Recall = \frac{TruePositives}{AllGroundTruths} = \frac{TP}{TP + FN}
$$

**_Ref:_** [知乎: AP，mAP 计算详解（代码全解）](https://zhuanlan.zhihu.com/p/70667071?utm_source=ZHShareTargetIDMore&utm_medium=social&utm_oi=37839882420224)

**AP (Average Precision)**

For a specific class, sort all predicted positive results. Use each predicted positive sample confidence as threshold to calculate a couple of Precision and Recall.

Draw a figure which x axis is Recall, y axis is Precision, and the area of bellow the curve is AP.

**mAP**

For multi class, calculate each class's AP, and average of these APs.

---

## NMS

### Implementation Reference

PyTorch version

**_References:_**

- [MMDetection: ops/nms](https://github.com/open-mmlab/mmdetection/blob/master/mmdet/ops/nms/src/nms_cpu.cpp)

- [知乎: 如何解释召回率与准确率？](https://www.zhihu.com/question/19645541/answer/91694636)

---

## Anchor

### Calculate loss of predict anchors and ground truth anchors

Ground truth: $y \in R^{(N_{y}, 4)}$

Predict: $\hat{y} \in R^{(N_{\hat{y}}, 4)}$

```python
bbox_a = torch.randn(3, 4) # Predict anchor: num_y_hat_anchor x 4
bbox_b = torch.randn(2, 4) # Ground truth anchor: num_y_anchor x 4
x2 = torch.min(bbox_a[:, 2].unsqueeze(1), bbox_b[:, 2])
# x2y2 = torch.min(bbox_a[:, [2, 3]].unsqueeze(1), bbox_b[:, [2, 3]]) # Calculate x2, y2 together
print(x2.size())
>>> torch.Size([3, 2]) # num_y_hat_anchor x num_y_anchor, each row: y_hat_anchor_i vs each  y_anchor x2
```

**_Code References:_**

- [Github yhenon/pytorch-retinanet](https://github.com/yhenon/pytorch-retinanet/blob/1135e18b835481b18fd0d4e1613c87afc2bc7d46/losses.py#L5)
- [Github amdegroot/ssd.pytorch](https://github.com/amdegroot/ssd.pytorch/blob/5b0b77faa955c1917b0c710d770739ba8fbff9b7/layers/box_utils.py#L29)

<!--  -->
<br>

---

<!--  -->

## HOG

**_Ref:_** [方向梯度直方图（HOG）](https://www.jianshu.com/p/6f69c751e9e7)

<!--  -->
<br>

---

<!--  -->

## Deformable Part Model

- [Source Code](http://www.rossgirshick.info/latent/)

### Blogs

- [CSDN: 目标检测（Object Detection）原理与实现(六)](https://blog.csdn.net/marvin521/article/details/9244193)
- [CSDN: DPM 目标检测算法(毕业论文节选)](https://blog.csdn.net/ttransposition/article/details/41806601)
- [CSDN: DPM(Deformable Parts Model)--原理(一)](https://blog.csdn.net/ttransposition/article/details/12966521)
- [CSDN: DPM（Deformable Part Model）原理详解](https://blog.csdn.net/qq_14845119/article/details/52625426)

<!--  -->
<br>

---

<!--  -->

## MMDetection

- [Github open-mmlab/mmdetection](https://github.com/open-mmlab/mmdetection)

### Framework

- backbone: usually a FCN network to extract feature maps, e.g., ResNet.
- neck: the part between backbones and heads, e.g., FPN, ASPP.
- head: the part for specific tasks, e.g., bbox prediction and mask prediction.
- roi extractor: the part for extracting features from feature maps, e.g., RoI Align.

**_Ref:_** [mmdetection/TECHNICAL_DETAILS.md](https://github.com/open-mmlab/mmdetection/blob/master/TECHNICAL_DETAILS.md#model)

<!--  -->
<br>

---

<!--  -->

## SSD

Code:

- [Github qfgaohao/pytorch-ssd](https://github.com/qfgaohao/pytorch-ssd)
- [Github amdegroot/ssd.pytorch](https://github.com/amdegroot/ssd.pytorch)

<!--  -->
<br>

---

<br>
<!--  -->

# Object Segmentation

## Blog

**_References:_**

- [Github horvitzs/Interactive_Segmentation_Models](https://github.com/horvitzs/Interactive_Segmentation_Models)

## Paper

### :+1:Large-scale interactive object segmentation with human annotators

- [CVPR 2019](https://arxiv.org/abs/1903.10830)

### Deep Extreme Cut: From Extreme Points to Object Segmentation

- [CVPR 2018](https://arxiv.org/abs/1711.09081)

- [Github PyTorch](https://github.com/scaelles/DEXTR-PyTorch)

- [Github TensorFlow](https://github.com/scaelles/DEXTR-PyTorch)

### Efficient Interactive Annotation of Segmentation Datasets with Polygon-RNN++

- [CVPR 2018](https://arxiv.org/abs/1803.09693)

- [Github TensorFlow](https://github.com/fidler-lab/polyrnn-pp)

- [Github PyTorch](https://github.com/fidler-lab/polyrnn-pp-pytorch)

Related blogs:

- [知乎: Polygon-RNN++图像分割数据集自动标注](https://zhuanlan.zhihu.com/p/42262628)

<!--  -->
<br>

---

<br>
<!--  -->

# Semantic Segmentation

## Tools

- [PyTorch Semantic Segmentation](https://github.com/hszhao/semseg)

<!--  -->
<br>

---

<br>
<!--  -->

# AutoML

## Survey

- [JMLR: Survey on Automated Machine Learning](https://arxiv.org/abs/1904.12054)

**_References:_**

- [机器之心: AutoML 研究综述：让 AI 学习设计 AI](https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&mid=2247489146&idx=4&sn=e3a45136cd9149ce08d81ece80dcddb8&chksm=f9a264f5ced5ede3e65cca95144cfa6fc789b3c605257c5ed4b9d1fa9efbd4d24868fefba141&mpshare=1&scene=1&srcid=#rd)
- [机器之心: 业界 | 进化算法 + AutoML，谷歌提出新型神经网络架构搜索方法](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739385&idx=2&sn=592ef223e200fc91ecdc787b6f4bc0b8&scene=21#wechat_redirect)

## Book

- [AutoML.org: AUTOML: METHODS, SYSTEMS CHALLENGES](https://www.automl.org/book/)

**_References:_**

- [极市平台: 《AutoML：方法，系统，挑战》新书免费下载](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247489038&idx=2&sn=65faa31e0430d2a2f245643edcee7174&chksm=ec1ffbf7db6872e1baea832c15637a625e8ddc023f9918867e238a949e84df30885708641fb5&mpshare=1&scene=1&srcid=#rd)

## Neural Architecture Search (NAS)

### Blog

- [极市平台: 干货 | 让算法解放算法工程师——NAS 综述](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247489280&idx=2&sn=6f357acd8bf281e182fabf16617f4b3d&chksm=ec1ffaf9db6873efc45ae12aed52728b3020a160a87812d06c557843e5313cbee04a5d9d30c3&mpshare=1&scene=1&srcid=#rd)

<!--  -->
<br>

---

<br>
<!--  -->

# Saliency Detection

## Survey

- [arXiv-2019: Salient Object Detection in the Deep Learning Era: An In-Depth Survey](https://arxiv.org/abs/1904.09146)

<!--  -->
<br>

---

<!--  -->

## Paper

### Review of Visual Saliency Detection with Comprehensive Information

- [arXiv-2018](https://arxiv.org/abs/1803.03391)

### PiCANet: Learning Pixel-wise Contextual Attention for Saliency Detection

- [CVPR-2018](https://arxiv.org/abs/1708.06433)

- [Github](https://github.com/Ugness/PiCANet-Implementation)

### Pyramid Feature Attention Network for Saliency detection

- [CVPR-2019](https://arxiv.org/abs/1903.00179)

- [Github](https://github.com/CaitinZhao/cvpr2019_Pyramid-Feature-Attention-Network-for-Saliency-detection)

<!--  -->
<br>

---

<!--  -->

## Example

- [pyimagesearch: OpenCV Saliency Detection](https://www.pyimagesearch.com/2018/07/16/opencv-saliency-detection/)

<!--  -->
<br>

---

<br>
<!--  -->

# Image Assessment

## Paper

### NIMA: Neural Image Assessment

- [Paper](https://arxiv.org/abs/1709.05424)
- [Github: unofficial version](https://github.com/titu1994/neural-image-assessment)

## Dataset

- [AVA](http://refbase.cvc.uab.es/files/MMP2012a.pdf)
  - [How to download ?](https://github.com/mtobeiyf/ava_downloader)

<!--  -->
<br>

---

<br>
<!--  -->

# Reinforcement Learning

## Survey

### Modern Deep Reinforcement Learning Algorithms

- [Paper](https://arxiv.org/abs/1906.10025)

**_Ref:_** [专知: 莫斯科国立大学 56 页《深度强化学习综述》最新论文，带你全面了解 DRL 最新方法](https://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247512884&idx=1&sn=39d39d974c6b7234f32092cf8f29c216&chksm=fc865227cbf1db31c04fc4da2cebdb89d8de815eaa7d1d7f349ac67fb9359f509e2e0e79aeb4&mpshare=1&scene=1&srcid=#rd)

## Blog

- [知乎-深度强化学习实现全家桶](https://zhuanlan.zhihu.com/p/68950847?utm_source=ZHShareTargetIDMore&utm_medium=social&utm_oi=37839882420224)

<!--  -->
<br>

---

<br>
<!--  -->

# 思想总结

- 拉格朗日乘数法：将约束的优化问题，通过一系列的变换，转化为无约束的问题，从而便于求解。

<!--  -->
<br>

---

<br>
<!--  -->

# Papers Follow-up

## CVPR 2019

- [Paper List](http://openaccess.thecvf.com/CVPR2019.py)

Others:

- [极市开发者社区: CVPR 2019 论文汇总（按方向划分，0611 更新中）](http://bbs.cvmart.net/topics/302/cvpr)
