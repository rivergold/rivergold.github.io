# Base

## P, NP, NP-Complelte and NP-Hard

***Referecne:*** [stackoverflow: What are the differences between NP, NP-Complete and NP-Hard?](https://stackoverflow.com/questions/1857244/what-are-the-differences-between-np-np-complete-and-np-hard)

<!--  -->
<br>

***
<!--  -->

## 堆排序

***References:***

- [CSDN: 堆树（最大堆、最小堆）详解](https://blog.csdn.net/guoweimelon/article/details/50904346)

- [Blog: 堆排序的时间复杂度](https://chihminh.github.io/2016/08/08/heap-sort/)

<!--  -->
<br>

***
<!--  -->

## 树

### 多叉树遍历

- 广度优先遍历: 使用队列
- 深度优先遍历: 使用栈

<!--  -->
<br>

***
<!--  -->

## 图

### 最短路径

#### Bellman-Ford

***References:***

- [掘金: 算法(五):图解贝尔曼-福特算法](https://juejin.im/post/5b77fec1e51d4538cf53be68)
- [博客园: Bellman-Ford 单源最短路径算法](https://www.cnblogs.com/gaochundong/p/bellman_ford_algorithm.html)

<!--  -->
<br>

***
<!--  -->

## 凸包

<!--  -->
<br>

***
<!--  -->

## Automatic Differentiation

计算机实现求导目前主要有两种方法：
- 基于符号的算术求导: 构建计算图
- 直接用数值进行求导: 使用求导公式，一般只用来检验求导结果

***Ref*** [Code码农网: CSE 599W： Systems for ML](https://www.codercto.com/a/29673.html) and [CSE599W: Lecture 4: Backpropagation and
Automatic Differentiation](http://dlsys.cs.washington.edu/pdf/lecture4.pdf)

***Referneces:***

- [知乎: tensorflow的函数自动求导是如何实现的？](https://www.zhihu.com/question/54554389/answer/164942272)

- [Github: dlsys-course/assignment1](https://github.com/dlsys-course/assignment1)

- [CSE 599W](http://dlsys.cs.washington.edu/pdf/lecture4.pdf)

### Tools

- [autograd](https://github.com/HIPS/autograd)

***References:***

- [ResearchGate: Is there an efficient automatic differentiation package in Python?](https://www.researchgate.net/post/Is_there_an_efficient_automatic_differentiation_package_in_Python)
- [autodiff.org](http://www.autodiff.org/?module=Tools&language=python)

<!--  -->
<br>

***
<!--  -->

## NMS

## Bias and Variance

```python
from numpy.linalg import norm
n_samples = 20
f_x, f_y = f(n_samples)
n_models = 100
max_degree = 15
var_vals =[]
bias_vals = []
error_vals = []
for degree in xrange(1, max_degree):
    avg_y = np.zeros(n_samples)
    models = []
    for i in xrange(n_models):
        (x,y) = sample(n_samples)
        model = fit_polynomial(x, y, degree)
        p_y = apply_polynomial(model, x)
        avg_y = avg_y + p_y
        models.append(p_y)
    avg_y = avg_y / n_models
    bias_2 = norm(avg_y - f_y)/f_y.size
    bias_vals.append(bias_2)
    variance = 0
    for p_y in models:
        variance += norm(avg_y - p_y)
    variance /= f_y.size * n_models
    var_vals.append(variance)
    error_vals.append(variance + bias_2)
pl.plot(range(1, max_degree), bias_vals, label='bias')
pl.plot(range(1, max_degree), var_vals, label='variance')
pl.plot(range(1, max_degree), error_vals, label='error')
pl.legend()
```

***Ref:*** [博客园: 非极大值抑制（NMS）的几种实现](https://www.cnblogs.com/king-lps/p/9031568.html)

<!--  -->
<br>

***
<!--  -->

## Linear Computation

### Tools

#### C++

- Better choose `Eigen` first

***References:***

- [知乎: 矩阵运算库blas, cblas, openblas, atlas, lapack, mkl之间有什么关系，在性能上区别大吗？](https://www.zhihu.com/question/27872849)
- [Blog: C++线性运算库梳理](https://milkpku.github.io/blog/2017/12/15/C++%E7%BA%BF%E6%80%A7%E8%BF%90%E7%AE%97%E5%BA%93%E6%A2%B3%E7%90%86/)


<!--  -->
<br>

***

<br>
<!--  -->

# Optimization

## Tools

- [Google OR-Tools](https://github.com/google/or-tools)

***Ref:*** [Github Awesome Optimization](https://github.com/jkerfs/awesome-optimization)

## Books

- [Convex Optimization](http://stanford.edu/~boyd/cvxbook/)

## Heuristic

- [Wiki: Heuristic (computer science)](https://en.wikipedia.org/wiki/Heuristic_(computer_science))

### Evolutionary Optimization

- [Pyswarm](https://pypi.org/project/pyswarm/): a gradient-free, evolutionary optimization package for python that supports constraints.

- [PySwarms](https://pyswarms.readthedocs.io/en/latest/): an extensible research toolkit for particle swarm optimization (PSO) in Python.

<!--  -->
<br>

***

<br>
<!--  -->

# Image Processing

## Resize Image

Here are 5 methods in OpenCV to resize image:

- `INTER_NEAREST`
- `INTER_LINEAR`
- `INTER_AREA`
- `INTER_CUBIC`
- `INTER_LANCZOS4`

When to use:

- Shrink image: `INTER_AREA` looks best
- Zoom image: `CV_INTER_CUBIC` looks best but slow, `CV_INTER_LINEAR` faster but still looks ok.

***Ref:*** [OpenCV Doc: resize](https://docs.opencv.org/3.4.6/da/d54/group__imgproc__transform.html#ga47a974309e9102f5f08231edc7e7529d)

***References:***

- [stackoverflow: How to Resize image without loosing image quality in c++ or opencv](https://stackoverflow.com/questions/33183272/how-to-resize-image-without-loosing-image-quality-in-c-or-opencv)

<!--  -->
<br>

***

<br>
<!--  -->

# Machine Learning

## Bias and Variance

```python
from numpy.linalg import norm
n_samples = 20
f_x, f_y = f(n_samples)
n_models = 100
max_degree = 15
var_vals =[]
bias_vals = []
error_vals = []
for degree in xrange(1, max_degree):
    avg_y = np.zeros(n_samples)
    models = []
    for i in xrange(n_models):
        (x,y) = sample(n_samples)
        model = fit_polynomial(x, y, degree)
        p_y = apply_polynomial(model, x)
        avg_y = avg_y + p_y
        models.append(p_y)
    avg_y = avg_y / n_models
    bias_2 = norm(avg_y - f_y)/f_y.size
    bias_vals.append(bias_2)
    variance = 0
    for p_y in models:
        variance += norm(avg_y - p_y)
    variance /= f_y.size * n_models
    var_vals.append(variance)
    error_vals.append(variance + bias_2)
pl.plot(range(1, max_degree), bias_vals, label='bias')
pl.plot(range(1, max_degree), var_vals, label='variance')
pl.plot(range(1, max_degree), error_vals, label='error')
pl.legend()
```

Ref [Github Gist: fabgoos/Bias and Variance.ipynb](https://gist.github.com/fabgoos/6788818)

***References:***

- [Liuchengxu Blog: 偏差与方差](http://liuchengxu.org/blog-cn/posts/bias-variance/)
- [Blog: Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)
- [知乎: 机器学习中的Bias和Variance](https://zhuanlan.zhihu.com/p/45213397)
- [Blog: 谈谈 Bias-Variance Tradeoff](https://liam.page/2017/03/25/bias-variance-tradeoff/)

<!--  -->
<br>

***
<!--  -->

## Ensemble learning

***References:***

- [知乎: 集成学习三大法宝-bagging、boosting、stacking](https://zhuanlan.zhihu.com/p/36161812)
- [Medium: Boosting, Bagging, and Stacking — Ensemble Methods with sklearn and mlens](https://medium.com/@rrfd/boosting-bagging-and-stacking-ensemble-methods-with-sklearn-and-mlens-a455c0c982de)


## Stacking

***References:*** [简书: Kaggle提升模型性能的超强杀招Stacking——机器学习模型融合](https://www.jianshu.com/p/719fc024c0ec)


# Deep Learning

## Basics

### How to calculate gradient in deep learning?

Key: use computation graphs.

使用计算图表示计算过程，在计算梯度时，基于链式求导法则，构建梯度的计算图。

**Computation Graphs:**

- [PyTorch Computation Graph](https://jdhao.github.io/2017/11/12/pytorch-computation-graph/)

<!--  -->
<br>

***
<!--  -->

### Image Dataset RGB Mean

Calculate all training set to get R, G, B means.

Ref [Github DrSleep/tensorflow-deeplab-resnet: How can I get IMG_MEAN of a custom dataset? #146](https://github.com/DrSleep/tensorflow-deeplab-resnet/issues/146)

<!--  -->
<br>

***

<br>
<!--  -->

# Object Detection

## Survey

- [TPAMI: Object Detection in 20 Years: A Survey](https://arxiv.org/pdf/1905.05055.pdf)

***References:***

- [专知: 密歇根大学40页《20年目标检测综述》最新论文，带你全面了解目标检测方法](https://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247511243&idx=1&sn=f2f7d013f4d7704eee3f0f94ffc1fbd1&chksm=fc864dd8cbf1c4ce4ef226d5f7c5a58b8ee9ccadfbd584e548154266f947d6e5dd7bfb6a25ed&mpshare=1&scene=1&srcid=#rd)

<!--  -->
<br>

***
<!--  -->

## MMDetection

- [Github open-mmlab/mmdetection](https://github.com/open-mmlab/mmdetection)

### Framework

- backbone: usually a FCN network to extract feature maps, e.g., ResNet.
- neck: the part between backbones and heads, e.g., FPN, ASPP.
- head: the part for specific tasks, e.g., bbox prediction and mask prediction.
- roi extractor: the part for extracting features from feature maps, e.g., RoI Align.

***Ref:*** [mmdetection/TECHNICAL_DETAILS.md](https://github.com/open-mmlab/mmdetection/blob/master/TECHNICAL_DETAILS.md#model)

<!--  -->
<br>

***
<!--  -->

## SSD

Code:

- [Github qfgaohao/pytorch-ssd](https://github.com/qfgaohao/pytorch-ssd)
- [Github amdegroot/ssd.pytorch](https://github.com/amdegroot/ssd.pytorch)

<!--  -->
<br>

***

<br>
<!--  -->

# AutoML

## Survey

- [JMLR: Survey on Automated Machine Learning](https://arxiv.org/abs/1904.12054)

***References:***

- [机器之心: AutoML研究综述：让AI学习设计AI](https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&mid=2247489146&idx=4&sn=e3a45136cd9149ce08d81ece80dcddb8&chksm=f9a264f5ced5ede3e65cca95144cfa6fc789b3c605257c5ed4b9d1fa9efbd4d24868fefba141&mpshare=1&scene=1&srcid=#rd)
- [机器之心: 业界 | 进化算法 + AutoML，谷歌提出新型神经网络架构搜索方法](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739385&idx=2&sn=592ef223e200fc91ecdc787b6f4bc0b8&scene=21#wechat_redirect)

## Book

- [AutoML.org: AUTOML: METHODS, SYSTEMS CHALLENGES](https://www.automl.org/book/)

***References:***

- [极市平台: 《AutoML：方法，系统，挑战》新书免费下载](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247489038&idx=2&sn=65faa31e0430d2a2f245643edcee7174&chksm=ec1ffbf7db6872e1baea832c15637a625e8ddc023f9918867e238a949e84df30885708641fb5&mpshare=1&scene=1&srcid=#rd)

<!--  -->
<br>

***

<br>
<!--  -->

# 思想总结

- 拉格朗日乘数法：将约束的优化问题，通过一系列的变换，转化为无约束的问题，从而便于求解。
